{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought process\n",
    "\n",
    "The directive is to build an LLM-based application that can automatically answer any of the previously answered graded assignment and return their corresponding answer.\n",
    "\n",
    "It wouldn't be as simple as returning the correct answer as given the instructions, it would most likely take new inputs and values and have the scripts run.\n",
    "\n",
    "Sample request\n",
    "\n",
    "```bash\n",
    "curl -X POST \"https://your-app.vercel.app/api/\" \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"question=Download and unzip file abcd.zip which has a single extract.csv file inside. What is the value in the \"answer\" column of the CSV file?\" \\\n",
    "  -F \"file=@abcd.zip\"\n",
    "```\n",
    "\n",
    "This project will rely on a [Waterfall Model](https://www.geeksforgeeks.org/waterfall-model/). The project needs to first idenify the following issues before it could move down to the next objective.\n",
    "\n",
    "It's been identified to ensure completion the following objectives must be addressed.\n",
    "\n",
    "1. Parsing the Python files to get the methods\n",
    "2. Allowing the LLM to understand which method to call based on the `question`\n",
    "3. Setting up the API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structure\n",
    "\n",
    "The project needs to be organized in a manner that's easily managable, thus the structure will require a folder to store all the modules, an app to serve as the API, a folder to source all the downloaded content, and a folder to contain all vector collections.\n",
    "\n",
    "```bash\n",
    "├───chromadb\n",
    "│   └───vector collections...\n",
    "├───data\n",
    "│   └───downloaded files...\n",
    "├───docs\n",
    "│   └───documentation purposes...\n",
    "├───submissions\n",
    "│   ├───consists of graded assignment modules...\n",
    "│   └───task.py\n",
    "├───.gitignore\n",
    "├───app.py\n",
    "├───pyproject.toml\n",
    "├───README.md\n",
    "└───uv.lock\n",
    "```\n",
    "\n",
    "\n",
    "### Function Calling vs Embeddings\n",
    "\n",
    "Function Calling might be the closest approach **HOWEVER** given the scope of the project, this will inevitably hit the token limit if each task were to be given their own funciton call. A much suited approach is to take a Retrieval Augmented Generation (RAG) to not only reference authorative knowledge but to keep the scope of the project within its capabilities. A refusal parameter can be added if the model cannot comprehend the request of the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Assignment Breakdowns\n",
    "\n",
    "This section focuses on each task for each graded assignment and their associated module within the project. This also serves as a checklist to idenfity which are possible within a Pythonic approach. __Minimize this section if necessary__.\n",
    "\n",
    "⚠️ Refers to a task that might rely on external control\n",
    "\n",
    "💣 Refers to a task that cannot be accomplished with just Python\n",
    "\n",
    "🚫 Refers to a task that cannot be accomplished\n",
    "\n",
    "- Graded Assignment 1\n",
    "  - [VS Code Version](../submissions/vscode_info.py) \n",
    "  - [Make HTTP Requests with uv](../submissions/uv_requests.py)\n",
    "  - [Run command with npx](../submissions/npx_prettier.py) ⚠️\n",
    "  - [Use Google Sheets](../submissions/google_sheets.py) 💣\n",
    "  - [Use Excel](../submission/excel.py) 💣\n",
    "  - [Use DevTools](../submission/chrome_devtools.py) 💣\n",
    "  - [Count Wednesdays](../submissions/counting_days.py)\n",
    "  - [Extract CSV from a ZIP](../submissions/zipfile_extract.py)\n",
    "  - [Use JSON](../submissions/sorting_students.py)\n",
    "  - [Multi-cursor edits](../submissions/json_cleanup.py)\n",
    "  - [CSS Selectors](../submissions/css_selectors.py)\n",
    "  - [Process files with different encoding](../submissions/process_encoding.py)\n",
    "  - [Use GitHub](../submissions/github_email.py) ⚠️\n",
    "  - [Replace across files](../submissions/replace_across.py) \n",
    "  - [List files and attributes](../submissions/sort_attributes.py)\n",
    "  - [Move and rename files](../submissions/move_rename.py)\n",
    "  - [Compare files](../submissions/compare_files.py) \n",
    "  - [SQL: Ticket Sales](../submissions/ticket_sales.py)\n",
    "- Graded Assignment 2\n",
    "  - [Write documentation in Markdown](../submissions/create_markdown.py)\n",
    "  - [Compress an image](../submissions/compress_image.py)\n",
    "  - [Use an Image Library in Google Colab](../submissions/image_colab.py) 💣\n",
    "  - [Deploy a Python API to Vercel](../submissions/vercel_api.py) ⚠️\n",
    "  - [Create a GitHub Action](../submissions/github_action.py) ⚠️\n",
    "  - [Push an image to Docker Hub](../submissions/docker_hub.py) 💣\n",
    "  - [Write a FastAPI server to serve data](../submissions/fastapi_server) \n",
    "  - [Run a local LLM with Llamafile and ngrok](../submissions/local_lllm) ⚠️\n",
    "- Graded Assignment 3\n",
    "  - [LLM Sentiment Analysis](../submissions/sentiment_analysis.py)\n",
    "  - [LLM Token Costs](../submissions/token_costs.py)\n",
    "  - [Generate addresses with LLM](../submissions/generate_addresses.py)\n",
    "  - [Base64 Encoding](../submissions/base64_encoding.py)\n",
    "  - [LLM Embeddings](../submissions/llm_embeddings.py)\n",
    "  - [Embedding Similarity](../submissions/cosine_similarity.py)\n",
    "  - [Vector Databases](../submissions/vector_databses.py)\n",
    "  - [Function Calling](../submissions/function_calling.py)\n",
    "  - [Prompt Engineering](../submissions/prompt_engineering.py) 🚫\n",
    "- Graded Assignment 4\n",
    "  - [Import HTML to Google Sheets](../submissions/html_google.py) 💣\n",
    "  - [Scrape IMDb Movies](../submissions/imdb_movies.py)\n",
    "  - [Wikipedia Outline](../submissions/wikiepedia_outline.py)\n",
    "  - [Scrape the BBC Weather API](../submissions/bbc_weather.py)\n",
    "  - [Find the bounding box of a city](../submissions/bounding_box.py)\n",
    "  - [Search Hacker News](../submissions/hacker_news.py)\n",
    "  - [Find newest GitHub User](../submissions/newest_user.py)\n",
    "  - [Scheduled GitHub Action](../submissions/github_actions.py)\n",
    "  - [Extract tables from PDF](../submissions/extract_tables.py)\n",
    "  - [Convert PDF to Markdown](../submissions/pdf_to_markdown.py)\n",
    "- Graded Assignment 5\n",
    "  - [Clean up Excel sales data](../submissions/clean_sales.py) 💣\n",
    "  - [Clean up student marks](../submissions/clean_student_marks.py)\n",
    "  - [Apache log requests](../submissions/log_requests.py) 💣\n",
    "  - [Apache log downloads](../submissions/log_request_downloads.py)\n",
    "  - [Parsing JSON](../submissions/parse_json.py)\n",
    "  - [Extract nested JSON keys](../submissions/extract_keys.py)\n",
    "  - [DuckDB: Social Media Interactions](../submissions/duckdb_interactions.py) ⚠️\n",
    "  - [Transcribe a Youtube video](../submissions/yt_transcribe.py) ⚠️\n",
    "  - [Reconstruct an image](../submissions/jigsaw_image.py) ⚠️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'compare_files',\n",
       "  'docstring': 'Compare two files line by line and count the number of differing lines.\\n\\nArgs:\\n    path1 (str): The file path of the first file to compare.\\n    path2 (str): The file path of the second file to compare.\\n\\nReturns:\\n    int: The number of lines that differ between the two files.\\n\\nRaises:\\n    FileNotFoundError: If the specified directory does not exist.\\n    \\nExample:\\n    >>> compare_files(\"a.txt\", \"b.txt\")\\n    38',\n",
       "  'params': ['path1', 'path2'],\n",
       "  'filepath': '../submissions/compare_files.py'},\n",
       " {'name': 'replace_across',\n",
       "  'docstring': '',\n",
       "  'params': ['path', 'replace_from', 'replace_to'],\n",
       "  'filepath': '../submissions/replace_across.py'},\n",
       " {'name': 'vscode_info',\n",
       "  'docstring': 'Returns the process usage and diagnostic information with vscode through the `code -s` command.\\n\\nReturns:\\n    str: The result from running `code -s` in the terminal.',\n",
       "  'params': [],\n",
       "  'filepath': '../submissions/vscode_info.py'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def parse_functions(directory: str) -> List[Dict]:\n",
    "    folder = os.listdir(directory)\n",
    "    functions = []\n",
    "    for function in folder:\n",
    "        if not function.endswith(\".py\"):\n",
    "            continue\n",
    "        func = os.path.join(directory, function)\n",
    "        with open(func, \"r\") as file:\n",
    "            tree = ast.parse(file.read())\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                doctring = ast.get_docstring(node) or \"\"\n",
    "                params = [arg.arg for arg in node.args.args]\n",
    "                functions.append({\n",
    "                    \"name\": node.name,\n",
    "                    \"docstring\": doctring,\n",
    "                    \"params\": params,\n",
    "                    \"filepath\": func\n",
    "                })\n",
    "    return functions\n",
    "\n",
    "functions = parse_functions(\"../submissions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PresistentClient(path=\"chromadb\")\n",
    "collection = client.get_or_create_collection(name=\"functions\")\n",
    "model = SentenceTransformer('all-distilroberta-v1')\n",
    "\n",
    "def generate_embeddings(functions: List[Dict]) -> List[Dict]:\n",
    "    for func in functions:\n",
    "        text = f\"Function {func[\"name\"]}: {func[\"docstring\"]} Parameters: {\" \".join(func[\"params\"])}\"\n",
    "        func[\"embedding\"] = model.encode(text)\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
