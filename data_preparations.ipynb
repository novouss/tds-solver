{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ WARNING\n",
    "\n",
    "This notebook isn't directly used for the API and is only intended for the development process of the project\n",
    "\n",
    "Please remove the following to ensure a clean production build\n",
    "\n",
    "```bash\n",
    "uv remove python-dotenv ipykernel ollama uvicorn tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought process\n",
    "\n",
    "The directive is to build an LLM-based application that can automatically answer any of the previously answered graded assignment and return their corresponding answer.\n",
    "\n",
    "It wouldn't be as simple as returning the correct answer as given the instructions, it would most likely take new inputs and values and have the scripts run.\n",
    "\n",
    "Sample request\n",
    "\n",
    "```bash\n",
    "curl -X POST \"https://your-app.vercel.app/api/\" \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"question=Download and unzip file abcd.zip which has a single extract.csv file inside. What is the value in the \"answer\" column of the CSV file?\" \\\n",
    "  -F \"file=@abcd.zip\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structure\n",
    "\n",
    "The project needs to be organized in a manner that's easily managable, thus the structure will require a folder to store all the modules, an app to serve as the API, a folder to source all the downloaded content, and a folder to contain all vector collections.\n",
    "\n",
    "```bash\n",
    "├───chromadb\n",
    "│   └───vector collections...\n",
    "├───helpers\n",
    "│   ├───helper functions...\n",
    "│   └───task.py\n",
    "├───data\n",
    "│   └───downloaded files...\n",
    "├───submissions\n",
    "│   ├───consists of graded assignment modules...\n",
    "│   └───task.py\n",
    "├───.gitignore\n",
    "├───app.py\n",
    "├───data_preparations.ipynb\n",
    "├───pyproject.toml\n",
    "├───README.md\n",
    "└───uv.lock\n",
    "```\n",
    "\n",
    "\n",
    "### Function Calling vs Embeddings\n",
    "\n",
    "Function Calling might be the closest approach **HOWEVER** given the scope of the project, this will inevitably hit the token limit if each task were to be given their own funciton call. A much suited approach is to take a Retrieval Augmented Generation (RAG) to not only reference authorative knowledge but to keep the scope of the project within its capabilities. A refusal parameter can be added if the model cannot comprehend the request of the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def parse_functions(directory: str) -> List[Dict]:\n",
    "    folder = os.listdir(directory)\n",
    "    functions = []\n",
    "    for function in folder:\n",
    "        if not function.endswith(\".py\"):\n",
    "            continue\n",
    "        func = os.path.join(directory, function)\n",
    "        with open(func, \"r\") as file:\n",
    "            tree = ast.parse(file.read())\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                docstring = ast.get_docstring(node)\n",
    "                params = [arg.arg for arg in node.args.args]\n",
    "                if docstring:\n",
    "                    functions.append({\n",
    "                        \"name\": node.name,\n",
    "                        \"docstring\": docstring,\n",
    "                        \"params\": params\n",
    "                    })\n",
    "    return functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'json_cleanup',\n",
       " 'docstring': 'Cleans up a JSON file by removing escaped quotes and returns the cleaned output as a string.\\n\\nArgs:\\n    path (str): The path to the JSON file to be cleaned\\n\\nReturns:\\n    str: The cleaned JSON data as a base64 encoded string\\n\\nRaises:\\n    FileNotFoundError: If the specified path does not exist\\n    json.JSONDecodeError: If the file is not a valid JSON\\n\\nExample:\\n    >>> json_cleanup(\"path/to/file.json\")\\n    \\'{\"Hello\": \"World\", ...}\\'',\n",
       " 'params': ['path']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions = parse_functions(\"./submissions/\")\n",
    "functions[random.randint(0, len(functions) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.authentication import generate_embeddings\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "collection = client.get_or_create_collection(name=\"functions\", metadata={ \"hnsw:space\": \"cosine\" })\n",
    "\n",
    "def create_embeddings(functions: List[Dict]) -> List[Dict]:\n",
    "    for func in tqdm(functions):\n",
    "        text = f\"Function {func[\"name\"]}: {func[\"docstring\"]} Parameters: {\" \".join(func[\"params\"])}\"\n",
    "        func[\"embedding\"] = generate_embeddings(text)\n",
    "    return functions\n",
    "\n",
    "def append_to_collection(functions: List[Dict]) -> None:\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    embeddings = []\n",
    "    ids = []\n",
    "    \n",
    "    for idx, func in enumerate(functions):\n",
    "        documents.append(func[\"name\"])\n",
    "        metadatas.append({ \"docstring\": func[\"docstring\"] })\n",
    "        embeddings.append(func[\"embedding\"])\n",
    "        ids.append(str(idx))\n",
    "\n",
    "    collection.add(\n",
    "        documents = documents,\n",
    "        metadatas = metadatas,\n",
    "        embeddings = embeddings,\n",
    "        ids = ids\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:29<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "embedded_functions = create_embeddings(functions)\n",
    "# embedded_functions[random.randint(0, len(embedded_functions) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_collection(embedded_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allowing the LLM to understand which method to call based on a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "collection = client.get_collection(name=\"functions\")\n",
    "\n",
    "def query_collection(query: str) -> Dict:\n",
    "    result = collection.query(\n",
    "        query_embeddings = generate_embeddings(query),\n",
    "        n_results = 1\n",
    "    )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['33']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['counting_days']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'docstring': \"Calculates the total number of day of the week between two dates.\\n\\nArgs:\\n    day (str): The day of the week to calculate for Mondays, Tues, Wed, THURS, fri, Saturday, or Sunday\\n    start (str): The start date in 'year-month-day' format\\n    end (str): The end date in 'year-month-day' format\\n\\nReturns:\\n    str: The total number of of day of the week between the two dates\\n\\nRaises:\\n    ValueError: If the day of the week is not one of 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', or 'sun'\\n\\nExample:\\n    >>> counting_days('wed', '2022-01-01', '2022-12-31')\\n    '13'\"}]],\n",
       " 'distances': [[0.6521538543993015]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_collection(\"How many Wednesdays are there in the date range 1986-11-29 to 2008-08-14?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstring to function call\n",
    "\n",
    "The OpenAI API only accepts functions within valid JSON data types, thus function calling can only contain the following types:\n",
    "\n",
    "- string\n",
    "- number\n",
    "- boolean\n",
    "- null/empty\n",
    "- object\n",
    "- array\n",
    "\n",
    "Source: https://community.openai.com/t/function-calling-parameter-types/268564/8\n",
    "\n",
    "### Dealing with Dict\n",
    "\n",
    "Dictionaries can be tricky for as the number of parameters is not certain. Instruct the program to consider dictionaries as strings, then use the `json.loads()` function to retrieve necessary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
