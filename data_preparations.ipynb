{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ WARNING\n",
    "\n",
    "This notebook isn't directly used for the API and is only intended for the development process of the project\n",
    "\n",
    "Please remove the following to ensure a clean production build\n",
    "\n",
    "```bash\n",
    "uv remove sentence_transformers openai python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought process\n",
    "\n",
    "The directive is to build an LLM-based application that can automatically answer any of the previously answered graded assignment and return their corresponding answer.\n",
    "\n",
    "It wouldn't be as simple as returning the correct answer as given the instructions, it would most likely take new inputs and values and have the scripts run.\n",
    "\n",
    "Sample request\n",
    "\n",
    "```bash\n",
    "curl -X POST \"https://your-app.vercel.app/api/\" \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"question=Download and unzip file abcd.zip which has a single extract.csv file inside. What is the value in the \"answer\" column of the CSV file?\" \\\n",
    "  -F \"file=@abcd.zip\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structure\n",
    "\n",
    "The project needs to be organized in a manner that's easily managable, thus the structure will require a folder to store all the modules, an app to serve as the API, a folder to source all the downloaded content, and a folder to contain all vector collections.\n",
    "\n",
    "```bash\n",
    "├───chromadb\n",
    "│   └───vector collections...\n",
    "├───data\n",
    "│   └───downloaded files...\n",
    "├───submissions\n",
    "│   ├───consists of graded assignment modules...\n",
    "│   └───task.py\n",
    "├───.gitignore\n",
    "├───app.py\n",
    "├───data_preparations.ipynb\n",
    "├───pyproject.toml\n",
    "├───README.md\n",
    "└───uv.lock\n",
    "```\n",
    "\n",
    "\n",
    "### Function Calling vs Embeddings\n",
    "\n",
    "Function Calling might be the closest approach **HOWEVER** given the scope of the project, this will inevitably hit the token limit if each task were to be given their own funciton call. A much suited approach is to take a Retrieval Augmented Generation (RAG) to not only reference authorative knowledge but to keep the scope of the project within its capabilities. A refusal parameter can be added if the model cannot comprehend the request of the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def parse_functions(directory: str) -> List[Dict]:\n",
    "    folder = os.listdir(directory)\n",
    "    functions = []\n",
    "    for function in folder:\n",
    "        if not function.endswith(\".py\"):\n",
    "            continue\n",
    "        func = os.path.join(directory, function)\n",
    "        with open(func, \"r\") as file:\n",
    "            tree = ast.parse(file.read())\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                doctring = ast.get_docstring(node) or \"\"\n",
    "                params = [arg.arg for arg in node.args.args]\n",
    "                functions.append({\n",
    "                    \"name\": node.name,\n",
    "                    \"docstring\": doctring,\n",
    "                    \"params\": params,\n",
    "                    \"filepath\": func\n",
    "                })\n",
    "    return functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = parse_functions(\"./submissions/\")\n",
    "functions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open AI Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "\n",
    "URL = \"https://llmfoundry.straive.com/openai/v1/embeddings\"\n",
    "KEY = os.environ[\"AIPROXY_TOKEN\"]\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer { KEY }\",\n",
    "}\n",
    "\n",
    "def get_embeddings(text):\n",
    "    data = {\n",
    "        \"model\": \"text-embedding-3-small\",\n",
    "        \"input\": text,\n",
    "    }\n",
    "    response = httpx.post(URL, headers=headers, json=data)\n",
    "    return response.json()[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "collection = client.get_or_create_collection(name=\"functions\")\n",
    "\n",
    "def generate_embeddings(functions: List[Dict]) -> List[Dict]:\n",
    "    for func in functions:\n",
    "        text = f\"Function {func[\"name\"]}: {func[\"docstring\"]} Parameters: {\" \".join(func[\"params\"])}\"\n",
    "        func[\"embedding\"] = get_embeddings(text)\n",
    "    return functions\n",
    "\n",
    "def append_to_collection(functions: List[Dict]) -> None:\n",
    "    for idx, func in enumerate(functions):\n",
    "        collection.add(\n",
    "            documents=func[\"name\"],\n",
    "            embeddings=func[\"embedding\"],\n",
    "            ids=str(idx)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_functions = generate_embeddings(functions)\n",
    "embedded_functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_collection(embedded_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allowing the LLM to understand which method to call based on a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import submissions as sub\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "collection = client.get_collection(name=\"functions\")\n",
    "\n",
    "print(sub.cosine_similarity(collection, \"Get all these files and give me the SHA256 of all their content combined?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
